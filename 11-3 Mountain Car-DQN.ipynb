{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward:  -2172.0 running reward: -2172\n",
      "episode: 100 total reward:  -245.0 running reward: -315\n",
      "episode: 200 total reward:  -305.0 running reward: -349\n",
      "episode: 300 total reward:  -726.0 running reward: -668\n",
      "episode: 400 total reward:  -15793.0 running reward: -1537\n",
      "episode: 500 total reward:  -347.0 running reward: -304\n",
      "episode: 600 total reward:  -628.0 running reward: -1035\n",
      "episode: 700 total reward:  -2643.0 running reward: -1579\n",
      "episode: 800 total reward:  -319.0 running reward: -479\n",
      "episode: 900 total reward:  -258.0 running reward: -315\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Implemented by Yinyu Jin \"\"\"\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# avoid the warning message\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "# hyperparameters\n",
    "alpha=0.99  # reward decay\n",
    "gamma = 0.99  # \n",
    "learning_rate = 0.01\n",
    "hidden_layer= 24\n",
    "episodes = 100\n",
    "replace_target_iter = 30\n",
    "memory_size=500\n",
    "batch_size = 128 # the number of data from the memory to learn every time\n",
    "update_net2_steps = 20 # we update net2 every 20 steps\n",
    "steps =5000\n",
    "epsilon_max = 0.9\n",
    "e_greedy_increment = None\n",
    "\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size): \n",
    "        super (DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_actions)\n",
    "        )\n",
    "\n",
    "        # build network\n",
    "        self.mls = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "   \n",
    "        # initialize zero memory [s, a ,r, s_]\n",
    "        self.memory = np.zeros((memory_size, state_space*2+2)) \n",
    "        \n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        # epsilon\n",
    "        self.epsilon =0 if e_greedy_increment is not None else epsilon_max\n",
    "         \n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            out = self.forward(torch.Tensor(state))\n",
    "            action = torch.argmax(out).data.item()\n",
    "        else:  \n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "        return action\n",
    "    \n",
    "    def store_memory(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        self.memory[self.memory_counter%memory_size][0:2] = s\n",
    "        self.memory[self.memory_counter%memory_size][2] = a\n",
    "        self.memory[self.memory_counter%memory_size][3] = r\n",
    "        self.memory[self.memory_counter%memory_size][4:6] = s_\n",
    "        self.memory_counter +=1\n",
    "        \n",
    "    def learn (self):\n",
    "        # check to replace target weights\n",
    "        if self.learn_step_counter % replace_target_iter ==0:\n",
    "            net2.load_state_dict(net.state_dict())\n",
    "            \n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > memory_size: # memory size 1000, batch size 500\n",
    "            sample_index = np.random.choice(memory_size, size = batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size = batch_size)\n",
    "        batch_memory = torch.Tensor(self.memory[sample_index,:])\n",
    "\n",
    "        #learn\n",
    "        q = net.forward(batch_memory[:,0:2]).gather(dim=1, index=batch_memory[:,2].long().unsqueeze(1))\n",
    "        q_next = net2.forward(batch_memory[:,4:6]).detach().max(1)[0].reshape(batch_size,1)\n",
    "        target_q = batch_memory[:,3].unsqueeze(1) + gamma*q_next\n",
    "        \n",
    "        loss = net.mls(q, target_q)\n",
    "        net.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        net.optimizer.step()\n",
    "                \n",
    "    def discounted_norm_rewards (self, rewards):\n",
    "        returns = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed (range(len(rewards))):\n",
    "            running_add = running_add * gamma + rewards[t]\n",
    "            returns[t] = running_add\n",
    "            \n",
    "        #normalized discounted rewards\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns)+eps)\n",
    "        return returns\n",
    "    \n",
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "all_rewards = []\n",
    "running_rewards = []\n",
    "\n",
    "\n",
    "net  = DeepQNetwork(state_space,action_space, hidden_layer)\n",
    "net2 = DeepQNetwork(state_space,action_space, hidden_layer)\n",
    "\n",
    "step=0\n",
    "for episode in range(1000): \n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "#     for step in range(steps):\n",
    "    while True:\n",
    "        action = net.choose_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # store every memory [s, a, r, s_] at each step\n",
    "        net.store_memory(state,action,reward,new_state)\n",
    "        state = new_state\n",
    "        \n",
    "        # when our memory is full\n",
    "        if step > 200 and step % 10 ==0:\n",
    "            net.learn()\n",
    "                            \n",
    "        if done: \n",
    "            returns = net.discounted_norm_rewards(rewards)\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "            running_rewards.append(np.mean(all_rewards[-30:]))\n",
    "            if episode % 100 ==0:\n",
    "                print('episode:', episode, 'total reward: ', all_rewards[-1], 'running reward:', int(running_rewards[-1]))\n",
    "                \n",
    "            break\n",
    "        step += 1\n",
    "#         env.render()\n",
    "        \n",
    "        \n",
    "plt.plot(all_rewards)\n",
    "plt.plot(running_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
